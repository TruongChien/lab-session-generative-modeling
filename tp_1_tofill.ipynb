{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training diffusion models\n",
    "\n",
    "In this notebook, we present an introduction to Denoising Diffusion Models\n",
    "We aim for simplicity so as to appeal and will not deal with the engineering technicalities.\n",
    "Most of the functions of this notebook are simpler versions than the ones used in the wild.\n",
    "This notebook is structured as follows.\n",
    "\n",
    "<!-- TOC -->\n",
    "\n",
    "- [Training diffusion models](#training-diffusion-models)\n",
    "  - [Creating datasets](#creating-datasets)\n",
    "  - [Creating the Ornstein-Uhlenbeck process](#creating-the-ornstein-uhlenbeck-process)\n",
    "  - [Investigating the forward diffusion](#creating-the-diffusion-datasets)\n",
    "  - [Creating the model, loss and optimizer](#creating-the-model-loss-and-optimizer)\n",
    "  - [Training the model](#training-the-model)   \n",
    "  - [Sampling from the model](#sampling-from-the-model)\n",
    "  - [Onto image datasets](#onto-image-datasets)\n",
    "\n",
    "<!-- /TOC -->\n",
    "\n",
    "This codebase is inspired from  <https://github.com/yang-song/score_sde>.\n",
    "\n",
    "In this notebook we use the JAX framework. We use the `haiku` package to handle networks and the `optax` package to handle optimization.\n",
    "\n",
    "For each cell, what you have to fill is marked with a `TODO`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cell of useful functions used throughout the code. Can be omitted at first read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "\n",
    "def batch_mul(a, b):\n",
    "    return jax.vmap(lambda a, b: a * b)(a, b)\n",
    "\n",
    "\n",
    "def batch_div(a, b):\n",
    "    return jax.vmap(lambda a, b: a / b)(a, b)\n",
    "\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    \"\"\"Collate function for numpy arrays (see https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html).\"\"\"\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(np.array(samples)) for samples in transposed]\n",
    "    \n",
    "\n",
    "def repeater(data_loader):\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training diffusion models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion models are a powerful class of generative models.\n",
    "The following introduction of Diffusion models rely on the Stochastic Differential Equation(SDE) formalism. First, consider a *forward noising* process\n",
    "\n",
    "$\\mathrm{d} \\mathbf{X}_t = - (1/2) \\mathbf{X}_t \\mathrm{d} t + \\mathrm{d} \\mathbf{B}_t, \\qquad \\mathbf{X}_0 \\sim \\pi \\\n",
    "    .$\n",
    "\n",
    "This process progressively adds noise to the data so as to for large value of $t$, $\\mathbf{X}_t$ is close to a Gaussian. The goal is then to *reverse* that process by considering a *backward denoising* process, $(\\mathbf{Y}_t)_{t \\in [0, T]} = (\\mathbf{X}_{T-t})_{t \\in [0, T]}$. It is remarkable that this process also satisfies a SDE given by\n",
    "\n",
    "$\\mathrm{d} \\mathbf{Y}_t = \\{(1/2) \\mathbf{Y}_t + \\nabla \\log p_{T-t}(\\mathbf{Y}_t)\\}\\mathrm{d} t + \\mathrm{d} \\mathbf{B}_t, \\qquad \\mathbf{Y}_0 \\sim \\pi_T, $\n",
    "\n",
    "where $p_t$ is the density of $\\mathbf{X}_t$. $\\nabla \\log p_t$ is called the *score*. In practice we approximate this backward dynamics by:\n",
    "* Discretizing the backward SDE using the Euler-Maruyama integrator.\n",
    "* Sampling from $\\mathbf{Y}_0 \\sim \\mathrm{N}(0, \\mathrm{Id})$ instead of $\\mathbf{Y}_0 \\sim \\pi_T$.\n",
    "* Approximating the score leveraging techniques from score-matching and deep learning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets\n",
    "\n",
    "Back to the [top](#diffusion-schrÃ¶dinger-bridges-in-the-wild)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by introducing the two data distributions we consider in this experiment. We restrict ourselves to a simple two-dimensional setting. The goal is to interpolate between the so-called `circle` distribution and a Gaussian distribution. Let us start by creating and visualising these two datasets.\n",
    "\n",
    "To create the data we use `sklearn`.\n",
    "\n",
    "To create the datasets we use `torch.utils.data.Dataset`.\n",
    "\n",
    "The target dataset (double circle) is stored in `target_ds` and the reference dataset (Gaussian $\\mathcal{N}(0,\\mathrm{Id})$) is stored in `reference_ds`.\n",
    "\n",
    "TODO: nothing, simply run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tueplots import bundles\n",
    "plt.rcParams.update(bundles.neurips2022())\n",
    "\n",
    "def load_twodim(num_samples: int,\n",
    "                dataset: str,\n",
    "                dimension=2) -> np.ndarray:\n",
    "    \"\"\"Create the two dimensional dataset array.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): number of samples.\n",
    "        dataset (str): identifier of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        sample (np.ndarray): (num_samples, 2) array of samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    if dataset == 'gaussian_centered':\n",
    "        sample = np.random.randn(num_samples, dimension)\n",
    "        sample = sample\n",
    "    \n",
    "    if dataset == 'gaussian_shift':\n",
    "        sample = np.random.randn(num_samples, 2)\n",
    "        sample = sample + 1.5\n",
    "\n",
    "    if dataset == 'circle':\n",
    "        X, y = datasets.make_circles(\n",
    "            n_samples=num_samples, noise=0.0, random_state=None, factor=.5)\n",
    "        sample = X * 4\n",
    "        \n",
    "    if dataset == 'scurve':\n",
    "        X, y = datasets.make_s_curve(\n",
    "            n_samples=num_samples, noise=0.0, random_state=None)\n",
    "        init_sample = X[:, [0, 2]]\n",
    "        scaling_factor = 2\n",
    "        sample = (init_sample - init_sample.mean()) / \\\n",
    "            init_sample.std() * scaling_factor\n",
    "        \n",
    "    return sample\n",
    "\n",
    "\n",
    "class TwoDimDataClass(Dataset):\n",
    "    \"\"\"Create the two dimensional dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_type (str): identifier of the dataset.\n",
    "        N (int): number of samples.\n",
    "        batch_size (int): batch size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_type: str,\n",
    "                 N: int,\n",
    "                 batch_size: int,\n",
    "                 dimension = 2\n",
    "                 ):\n",
    "\n",
    "        self.X = load_twodim(N, dataset_type, dimension=dimension)\n",
    "        self.name = dataset_type\n",
    "        self.batch_size = batch_size\n",
    "        self.dimension = 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "    \n",
    "    def get_dataloader(self, shuffle=True):\n",
    "        return DataLoader(\n",
    "            self,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    \n",
    "target_ds = TwoDimDataClass(dataset_type='circle', \n",
    "                              N=1000000, \n",
    "                              batch_size=256)\n",
    "\n",
    "reference_ds = TwoDimDataClass(dataset_type='gaussian_centered',\n",
    "                              N=1000000,\n",
    "                              batch_size=256)\n",
    "\n",
    "Ntest = 500\n",
    "sample_f = target_ds[0:Ntest]\n",
    "sample_b = reference_ds[0:Ntest]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(sample_f[:, 0], sample_f[:, 1], alpha=0.6)\n",
    "ax.scatter(sample_b[:, 0], sample_b[:, 1], alpha=0.6)\n",
    "ax.grid(False)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "strtitle = \"Target and reference datasets\"\n",
    "ax.set_title(strtitle)\n",
    "ax.legend(['Datasest (target)', 'Dataset (reference)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Ornstein-Uhlenbeck process\n",
    "\n",
    "Back to the [top](#training-diffusion-models)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward process is given by \n",
    "\n",
    "$\\mathrm{d} \\mathbf{X}_t = - (1/2) \\mathbf{X}_t \\mathrm{d} t + \\mathrm{d} \\mathbf{B}_t, \\qquad \\mathbf{X}_0 \\sim \\pi \\; .$\n",
    "\n",
    "This process is known as the *Ornstein-Uhlenbeck* process and possesses a lot of appealing properties (geometric ergodicity, explicit transition kernel, explicit diagonalization basis etc.). In what follows, we create a `SDE` object. The most important method of `SDE` is `reverse` which creates the backward SDE associated with the current SDE. For example in our case, given the class associated with $(\\mathbf{X}_t)_{t \\in [0,T]}$ `reverse` would return the class associated with \n",
    "\n",
    "$\\mathrm{d} \\mathbf{Y}_t = \\{ (1/2) \\mathbf{Y}_t  + \\nabla \\log p_{T-t}(\\mathbf{Y}_t)\\}\\mathrm{d} t + \\mathrm{d} \\mathbf{B}_t, \\qquad \\mathbf{Y}_0 \\sim \\pi_T \\; ,$\n",
    "\n",
    "TODO: your task is to complete the `get_reverse_drift_fn` method in the `SDE` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from typing import Any\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "class SDE(abc.ABC):\n",
    "    \"\"\"SDE abstract class. Functions are designed for a mini-batch of inputs.\n",
    "\n",
    "    Args:\n",
    "      beta_schedule (Any): Schedule used for the SDE.\n",
    "      N (int): Number of discretization time steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 N: int):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def T(self):\n",
    "        \"\"\"End time of the SDE.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sde_coeff(self, t, x):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def marginal_prob(self, t, x):\n",
    "        \"\"\"Parameters to determine the marginal distribution of the SDE, $p_t(x)$.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def prior_sampling(self, rng, shape):\n",
    "        \"\"\"Generate one sample from the prior distribution, $p_T(x)$.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reverse(self,\n",
    "                model: Any,\n",
    "                params: Any) -> Any:\n",
    "        \"\"\"Create the reverse-time SDE/ODE.\n",
    "\n",
    "        Args:\n",
    "          model (Any): the network used (see config/network).\n",
    "          params (Any): the parameters of the networks.\n",
    "\n",
    "        Returns:\n",
    "          sde_backward (Any): the backward SDE.\n",
    "        \"\"\"\n",
    "        N = self.N\n",
    "        T = self.T\n",
    "        sde_coeff = self.sde_coeff\n",
    "\n",
    "        def get_model_fn(model, params):\n",
    "            key = jax.random.PRNGKey(0)\n",
    "            def model_fn(t, x):\n",
    "                return model.apply(params, key, t, x)\n",
    "            return model_fn\n",
    "        # NOTE: model_fn parameterizes the score.\n",
    "\n",
    "        def get_reverse_drift_fn(model_fn):\n",
    "            def reverse_drift_fn(t, x):\n",
    "                # TO FILL\n",
    "                ### \n",
    "                ###\n",
    "                ###\n",
    "                return reverse_drift\n",
    "            return reverse_drift_fn\n",
    "\n",
    "        # Build the class for reverse-time SDE.\n",
    "\n",
    "        class RSDE(self.__class__):\n",
    "            def __init__(self):\n",
    "                self.N = N\n",
    "                self.param_drift = 'score'\n",
    "                self.model_fn = get_model_fn(model, params)\n",
    "                self.reverse_drift_fn = get_reverse_drift_fn(self.model_fn)\n",
    "\n",
    "            @property\n",
    "            def T(self):\n",
    "                return T\n",
    "\n",
    "            def sde_coeff(self, t, x):\n",
    "                \"\"\"Create the drift and diffusion functions for the reverse SDE/ODE.\"\"\"\n",
    "                _, diffusion = sde_coeff(self.T-t, x)\n",
    "                drift = self.reverse_drift_fn(t, x)\n",
    "                return drift, diffusion\n",
    "\n",
    "        sde_backward = RSDE()\n",
    "        return sde_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the general class `SDE` we are now going to build a SDE based on the Ornstein-Uhlenbeck process.\n",
    "\n",
    "Recall that the Ornstein-Uhlenbeck (also called VP-SDE) is defined by \n",
    "$$ \\mathrm{d} \\mathbf{X}_t = - (1/2) \\mathbf{X}_t \\mathrm{d} t + \\mathrm{d} \\mathbf{B}_t$$\n",
    "\n",
    "Given $\\mathbf{X}_0$ and $t$ we can sample $\\mathbf{X}_t$ using that (in distribution) we have\n",
    "\n",
    "$$\\mathbf{X}_t = \\exp[-t] \\mathbf{X}_0 + (1 - \\exp[-2t])^{1/2} Z,$$\n",
    "\n",
    "with $Z \\sim \\mathcal{N}(0, \\mathrm{Id})$.\n",
    "\n",
    "TODO: you need to fill `sde_coeff`, `marginal_prob` and `prior_sampling`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeck(SDE):\n",
    "    def __init__(self,\n",
    "                 N=100):\n",
    "        \"\"\"Construct an Ornstein-Uhlenbeck SDE.\"\"\"\n",
    "\n",
    "        super().__init__(N)\n",
    "        self.N = N\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        return 1\n",
    "\n",
    "    def sde_coeff(self, t, x):\n",
    "        # TO FILL. Outputs: drift, diffusion. The drift and the matrix diffusion coefficients\n",
    "        ###\n",
    "        ###\n",
    "        return drift, diffusion\n",
    "\n",
    "    def marginal_prob(self, t, x):\n",
    "        # TO FILL. Ouputs: mean, std. The mean and the std to sample X_t.\n",
    "        ###\n",
    "        ###\n",
    "        ###\n",
    "        return mean, std\n",
    "\n",
    "    def prior_sampling(self, key, shape):\n",
    "        # TO FILL. Sample from the prior distribution.\n",
    "        return ###\n",
    "\n",
    "\n",
    "sde = OrnsteinUhlenbeck()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the forward diffusion\n",
    "\n",
    "Back to the [top](#creating-datasets)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize a few trajectories from the Ornstein-Uhlenbeck process $\\mathrm{d} \\mathbf{X}_t = - (1/2) \\mathbf{X}_t \\mathrm{d} t + \\mathrm{d} \\mathbf{B}_t$. Similarly, we can also plot the mean and standard deviation of the forward trajectories which should converge to $0$, respectively $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "n_arr = 10\n",
    "plot_Ntest = 100\n",
    "\n",
    "t_arr = jnp.linspace(0, sde.T, n_arr)\n",
    "mean_arr = jnp.zeros((n_arr, 1))\n",
    "std_arr = jnp.zeros((n_arr, 1))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for k, tt in enumerate(t_arr):\n",
    "    x0 = target_ds.__getitem__(range(plot_Ntest))\n",
    "    t = jnp.zeros((x0.shape[0],)) + tt\n",
    "    mean, std = sde.marginal_prob(t, x0)\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    z = jax.random.normal(key, mean.shape)\n",
    "    xt = mean + batch_mul(std, z)\n",
    "\n",
    "    mean_arr = mean_arr.at[k].set(jnp.sum(xt.mean(axis=0)**2))\n",
    "    std_arr = std_arr.at[k].set(\n",
    "        jnp.sum((xt.std(axis=0) - 1)**2))\n",
    "\n",
    "    ax.scatter(xt[:, 0], xt[:, 1], alpha=0.6, color=cmap(tt))\n",
    "\n",
    "ax.grid(False)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(t_arr, jnp.abs(mean_arr))\n",
    "plt.title(\"Distance of mean from 0\")\n",
    "ax.grid(False)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "plt.plot(t_arr, std_arr)\n",
    "plt.title(\"Distance of standard deviation from 1\")\n",
    "ax.grid(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model, loss and optimizer\n",
    "\n",
    "Back to the [top](#creating-datasets)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we instantiate the model which is going to be used to approximate the score $\\nabla \\log p_t$. Since, we are considering a low dimensional example, we simply transform the spatial and temporal input using Multi Layer Perceptron (MLP) (note that we get embedding for the time input using positional encoding). We then concatenate these embeddings and apply another layer of MLP.\n",
    "\n",
    "TODO: nothing, simply run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import haiku as hk\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def get_timestep_embedding(timesteps: jnp.ndarray,\n",
    "                           embedding_dim: int,\n",
    "                           max_positions=10000) -> jnp.ndarray:\n",
    "    \"\"\" Get timesteps embedding. \n",
    "    Function extracted from https: // github.com/hojonathanho/diffusion/blob/master/diffusion_tf/nn.py\n",
    "\n",
    "    Args:\n",
    "        timesteps (jnp.ndarray): timesteps array (Nbatch,).\n",
    "        embedding_dim (int): Size of the embedding.\n",
    "        max_positions (int, optional): _description_. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        emb (jnp.ndarray): embedded timesteps (Nbatch, embedding_dim).\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(max_positions) / (half_dim - 1)\n",
    "    emb = jnp.exp(jnp.arange(half_dim, dtype=jnp.float32) * -emb)\n",
    "    emb = timesteps[:, None] * emb[None, :]\n",
    "    emb = jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], axis=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = jnp.pad(emb, [[0, 0], [0, 1]])\n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "    return emb \n",
    "\n",
    "@dataclass\n",
    "class MLP:\n",
    "    hidden_shapes: list\n",
    "    output_shape: list\n",
    "    bias: bool = True\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for hs in self.hidden_shapes:\n",
    "            x = hk.Linear(output_size=hs, with_bias=self.bias)(x)\n",
    "            x = jnp.sin(x)\n",
    "\n",
    "        x = hk.Linear(output_size=self.output_shape)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Naive(hk.Module):\n",
    "    \"\"\"Create a naive MLP network.\n",
    "\n",
    "    Args:\n",
    "        output_shape (int): output shape.\n",
    "        enc_shapes (int): The shapes of the encoder.\n",
    "        t_dim (int): the dimension of the time embedding.\n",
    "        dec_shapes (int): The shapes of the decoder \n",
    "        resnet (bool): if True then the network is a resnet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_shape: int,\n",
    "        enc_shapes: int,\n",
    "        t_dim: int,\n",
    "        dec_shapes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.temb_dim = t_dim\n",
    "        t_enc_dim = t_dim * 2\n",
    "\n",
    "        self.net = MLP(hidden_shapes=dec_shapes,\n",
    "                       output_shape=output_shape)\n",
    "\n",
    "        self.t_encoder = MLP(\n",
    "            hidden_shapes=enc_shapes, output_shape=t_enc_dim\n",
    "        )\n",
    "\n",
    "        self.x_encoder = MLP(\n",
    "            hidden_shapes=enc_shapes, output_shape=t_enc_dim\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, x):\n",
    "        t = jnp.array(t, dtype=float)\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        temb = get_timestep_embedding(t.reshape(-1), self.temb_dim)\n",
    "        temb = self.t_encoder(temb)\n",
    "        xemb = self.x_encoder(x)\n",
    "        temb = jnp.broadcast_to(temb, [xemb.shape[0], *temb.shape[1:]])\n",
    "        h = jnp.concatenate([xemb, temb], -1)\n",
    "        out = -self.net(h)\n",
    "        return out\n",
    "    \n",
    "\n",
    "def init_model(key, batch_size=64, dimension=2):\n",
    "    \"\"\" Initialize a model. \"\"\"\n",
    "    def forward(t, x):\n",
    "        model = Naive(output_shape=dimension,\n",
    "                    enc_shapes=[32, 32],\n",
    "                    t_dim=16,\n",
    "                    dec_shapes=[32,32])\n",
    "        return model(t, x)\n",
    "    model = hk.transform(forward)\n",
    "\n",
    "    input_shape = (batch_size, dimension)\n",
    "    t_shape = (batch_size, 1)\n",
    "    dummy_t = jnp.zeros(t_shape)\n",
    "    dummy_input = jnp.zeros(input_shape)\n",
    "\n",
    "    init_params = model.init(key, t=dummy_t, x=dummy_input)\n",
    "    return model, init_params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "model, init_params = init_model(key, batch_size=target_ds.batch_size, dimension=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is instantiated we are ready to initialize the optimizer and the loss.\n",
    "\n",
    "TODO: write the loss in `loss_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "class DSMLoss():\n",
    "    \"\"\"Compute the Denoising Score Matching loss.\n",
    "\n",
    "    Args:\n",
    "        alpha (float): regularization parameter.\n",
    "        sde (Any): SDE object.\n",
    "        diff_weight (bool): whether to weight the loss by the diffusion.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: float,\n",
    "                 diff_weight: bool):\n",
    "        self.alpha = alpha\n",
    "        self.diff_weight = diff_weight\n",
    "\n",
    "    def loss_fn(self, params, key, model, batch):\n",
    "        t, xt, pred, diffusion_sq = batch\n",
    "        # TO FILL\n",
    "        out = model.apply(params, key, t, xt).squeeze()\n",
    "        l2loss = optax.l2_loss(out, pred.squeeze())\n",
    "        reg = out ** 2\n",
    "        loss = l2loss + self.alpha * reg\n",
    "        if self.diff_weight:\n",
    "            loss = loss / diffusion_sq.squeeze()\n",
    "        loss = jnp.mean(loss)\n",
    "        return loss\n",
    "\n",
    "loss_fn = DSMLoss(alpha=0, diff_weight=False).loss_fn\n",
    "optimizer = optax.adam(learning_rate=1e-3, b1=.9, b2=0.999, eps=1e-8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the updating step `step_fn` to be used in the training loop.\n",
    "\n",
    "TODO: write the batch to be given to `loss_fn`. This batch `batch_tot` is composed of \n",
    "* `t`: a time randomly sampled between $\\varepsilon$ and $1$ (you can pick the value of $\\varepsilon$)\n",
    "* `xt`: the value of $\\mathbf{X}_t$ given one element of the batch `batch` (corresponding to $\\mathbf{X}_0$) and a time `t`.\n",
    "* `pred`: the value to be predicted in the loss. This corresponds to $\\nabla \\log p_{t|0}(\\mathbf{X}_t|\\mathbf{X}_0)$ where $p_{t|0}$ is the distribution of $\\mathbf{X}_t|\\mathbf{X}_0$.\n",
    "* `diffusion_sq`: the square of the diffusion coefficients (only used if we rescaled the loss, so can be omitted at first read)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_fn(loss_fn: Any,\n",
    "                optimizer: Any,\n",
    "                sde: Any,\n",
    "                model: Any) -> Any:\n",
    "    \"\"\"Create one step of optimization.\n",
    "\n",
    "    Args:\n",
    "        loss_fn (Any): The loss function (see config/loss).\n",
    "        optimizer (Any): The optimizer (see config/optim).\n",
    "        model (Any): The neural network (see config/network).\n",
    "\n",
    "    Returns:\n",
    "        step_fn (Any): The learning state function.\n",
    "    \"\"\"\n",
    "\n",
    "    def step_fn(key: jax.random.KeyArray,\n",
    "                state: Any,\n",
    "                batch: Any) -> Any:\n",
    "        \"\"\"Apply one step of optimization\n",
    "\n",
    "        Args:\n",
    "            key (jax.random.KeyArray): random key\n",
    "            state (Any): training state\n",
    "            batch (Any): batch of data\n",
    "\n",
    "        Returns:\n",
    "            state (Any): updated training state.\n",
    "        \"\"\"\n",
    "        key, step_key = jax.random.split(key)\n",
    "        \n",
    "        # TO FILL\n",
    "        ### \n",
    "        ### \n",
    "        ### \n",
    "        \n",
    "        params = state.params\n",
    "        loss, grad = jax.value_and_grad(loss_fn)(\n",
    "            params, step_key, model, batch_tot)\n",
    "        update, new_opt_state = optimizer.update(grad, state.opt_state)\n",
    "        new_params = optax.apply_updates(params, update)\n",
    "\n",
    "        new_params_ema = jax.tree_map(\n",
    "            lambda p_ema, p: p_ema * state.ema_rate\n",
    "            + p * (1.0 - state.ema_rate),\n",
    "            state.params_ema,\n",
    "            new_params,\n",
    "        )\n",
    "        step = state.step + 1\n",
    "        new_state = state.replace(step=step+1,\n",
    "                                  opt_state=new_opt_state,\n",
    "                                  params=new_params,\n",
    "                                  params_ema=new_params_ema)\n",
    "\n",
    "        return new_state, loss, key\n",
    "    return step_fn\n",
    "\n",
    "\n",
    "step_fn = get_step_fn(loss_fn, optimizer, sde, model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Back to the [top](#creating-datasets)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate the `DiffusionState` object which keeps track of all the information related to the SDE we consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class DiffusionState:\n",
    "    dataclass: Any\n",
    "    step_fn: Any\n",
    "\n",
    "diffusion = DiffusionState(dataclass=target_ds,\n",
    "                             step_fn=step_fn)\n",
    "\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class State:\n",
    "    step: int\n",
    "    opt_state: Any\n",
    "    params: Any\n",
    "    ema_rate: float\n",
    "    params_ema: Any\n",
    "\n",
    "\n",
    "step = 0\n",
    "opt_state = optimizer.init(init_params)\n",
    "params = init_params\n",
    "params_ema = init_params\n",
    "\n",
    "state = State(step=step,\n",
    "              opt_state=opt_state,\n",
    "              params=params,\n",
    "              ema_rate=0.99,\n",
    "              params_ema=params_ema)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: write the training loop in `train_diffusion`. Note that you can use `jax.jit(diffusion.step_fn)` to compile one training step and greatly improve the speed of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_diffusion(diffusion, state, steps_total):\n",
    "    ###\n",
    "    ###\n",
    "    ###\n",
    "    return state\n",
    "\n",
    "steps_total = 10000\n",
    "state = train_diffusion(diffusion, state, steps_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the model\n",
    "\n",
    "Back to the [top](#creating-datasets)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained we can sample approximately from the backward SDE\n",
    "\n",
    "$\\mathrm{d} \\mathbf{Y}_t = \\{(1/2) \\mathbf{Y}_t + \\nabla \\log p_{T-t}(\\mathbf{Y}_t)\\}\\mathrm{d} t + \\mathrm{d} \\mathbf{B}_t, \\qquad \\mathbf{Y}_0 \\sim \\pi_T, $\n",
    "\n",
    "TODO: write `update_fn` in the `EulerMaruyamaPredictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import abc\n",
    "\n",
    "class Predictor(abc.ABC):\n",
    "    \"\"\"The abstract class for a predictor algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, sde):\n",
    "        super().__init__()\n",
    "        self.sde = sde\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update_fn(self,\n",
    "                  key: jax.random.KeyArray,\n",
    "                  t: jnp.ndarray,\n",
    "                  x: jnp.ndarray) -> Any:\n",
    "        \"\"\"One update of the predictor.\n",
    "\n",
    "        Args:\n",
    "          key (jax.random.KeyArray): A JAX random state.\n",
    "          t (jnp.ndarray): A JAX array representing the current time step.\n",
    "          x (jnp.ndarray): A JAX array representing the current state          \n",
    "\n",
    "        Returns:\n",
    "          x (jnp.ndarray): A JAX array of the next state.\n",
    "          x_mean (jnp.ndarray): A JAX array. The next state without random noise. Useful for denoising.\n",
    "          pred (jnp.ndarray): a JAX array. The prediction used in the loss function.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class EulerMaruyamaPredictor(Predictor):\n",
    "    def __init__(self, sde):\n",
    "        super().__init__(sde)\n",
    "        self.param_drift = self.sde.param_drift\n",
    "\n",
    "    def update_fn(self, key, t, x):\n",
    "      # TO FILL (x_mean is simply the output without the noise)\n",
    "      ###\n",
    "      ###\n",
    "      return x, x_mean\n",
    "\n",
    "class Sampler():\n",
    "    \"\"\"Create a sampling class.\n",
    "\n",
    "    Args:\n",
    "      eps (float): A float representing the epsilon stopping time for the backward diffusion.      \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 eps: float):\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_sampling_fn(self,\n",
    "                        sde: Any,\n",
    "                        dataset: Any) -> Any:\n",
    "        \"\"\"Create a sampling function.\n",
    "\n",
    "        Args:\n",
    "          sde (Any): An SDE.\n",
    "          dataset (Any): The dataset associated with this SDE (initialization).\n",
    "\n",
    "        Returns:\n",
    "          sampling_fn (Any): A sampling function which takes as input a random key and a number of samples.\n",
    "        \"\"\"\n",
    "        \n",
    "        update_fn = EulerMaruyamaPredictor(sde).update_fn\n",
    "\n",
    "        def sampling_fn(key: jax.random.KeyArray, N_samples: int) -> Any:\n",
    "            \"\"\"_summary_\n",
    "\n",
    "            Args:\n",
    "                key (jax.random.KeyArray): A JAX random state.\n",
    "                N_samples (int): The number of samples.\n",
    "\n",
    "            Returns:\n",
    "                out (jnp.ndarray): the last sample of the SDE (N_samples, dimension).\n",
    "                ntot (int): the total number of steps of the sampling procedure.\n",
    "                timesteps (jnp.ndarray): the timesteps of the sampling procedure.\n",
    "                x_hist (jnp.ndarray): the total history of the SDE (N, N_samples, dimension) (N is the number of timesteps).\n",
    "                pred_hist (jnp.ndarray): the total history of the predictions of the SDE (N, N_samples, dimension) (N is the number of timesteps).\n",
    "            \"\"\"\n",
    "            # Initial sample\n",
    "            x = dataset.__getitem__(range(N_samples))\n",
    "            timesteps = jnp.linspace(0, sde.T-self.eps, sde.N)\n",
    "\n",
    "            def loop_body(i, val):\n",
    "                key, x, x_mean, x_hist = val\n",
    "                t = timesteps[i]\n",
    "                vec_t = jnp.ones((x.shape[0], 1)) * t\n",
    "                key, step_key = random.split(key)\n",
    "                x, x_mean = update_fn(step_key, vec_t, x)\n",
    "                x_hist = x_hist.at[i].set(x)\n",
    "                return key, x, x_mean, x_hist\n",
    "\n",
    "            # Get shape for predictions update\n",
    "            t = jnp.ones((x.shape[0], 1))\n",
    "\n",
    "            x_hist = jnp.zeros((sde.N, *x.shape))\n",
    "            _, x, _, x_hist = jax.lax.fori_loop(\n",
    "                0, sde.N, loop_body, (key, x, x, x_hist))\n",
    "            out = x\n",
    "            ntot = sde.N\n",
    "            return out, ntot, timesteps, x_hist\n",
    "\n",
    "        return sampling_fn\n",
    "\n",
    "\n",
    "sde_backward = sde.reverse(model, state.params_ema)\n",
    "sampler = Sampler(eps=1e-3)\n",
    "sampler_fn = sampler.get_sampling_fn(sde_backward, reference_ds)\n",
    "\n",
    "out, ntot, timesteps, x_hist = sampler_fn(key, N_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "t_arr = jnp.linspace(0, 1, plot_Ntest)\n",
    "for k, tt in enumerate(t_arr):\n",
    "    outk = x_hist[k]\n",
    "    ax.scatter(outk[:, 0], outk[:, 1], alpha=0.3, color=cmap(tt))\n",
    "ax.grid(False)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "strtitle = \"Backward run\"\n",
    "ax.set_title(strtitle)\n",
    "\n",
    "xmin, xmax = -4, 4\n",
    "ymin, ymax = -4, 4\n",
    "xx, yy = jnp.mgrid[xmin:xmax:10j, ymin:ymax:10j]\n",
    "fig = plt.figure()\n",
    "t0 = 0.05\n",
    "x = jnp.concatenate(\n",
    "    (xx.reshape(-1, 1), yy.reshape(-1, 1)), axis=-1)\n",
    "t = jnp.zeros((x.shape[0],)) + t0\n",
    "out = model.apply(state.params_ema, key, t, x)\n",
    "u = out[:, 0].reshape(xx.shape)\n",
    "v = out[:, 1].reshape(yy.shape)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.quiver(xx, yy, u, v)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "strtitle = \"Vector field at time t=0.05\"\n",
    "ax.set_title(strtitle)\n",
    "\n",
    "out_true = target_ds[range(0, plot_Ntest)]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(x_hist[-1, :, 0], x_hist[-1, :, 1], alpha=0.6)\n",
    "ax.scatter(out_true[:, 0], out_true[:, 1], alpha=0.6)\n",
    "ax.grid(False)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "strtitle = \"Final backward particles\"\n",
    "ax.set_title(strtitle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onto image datasets\n",
    "\n",
    "Back to the [top](#creating-datasets)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "transforms_list = [\n",
    "    transforms.Pad(2, fill=0),  # left and right 2+2=4 padding\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "]\n",
    "\n",
    "mnist_ds = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transforms.Compose(transforms_list))\n",
    "images = np.array(torch.concatenate([mnist_ds[i][0] for i in range(len(mnist_ds))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and std\n",
    "mean, std = images.mean(), images.std()\n",
    "print(f'Mean: {mean:.3f}, Standard Deviation: {std:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataClass(Dataset):\n",
    "    \"\"\"Create a flatten image dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_type (str): identifier of the dataset.\n",
    "        N (int): number of samples.\n",
    "        batch_size (int): batch size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_ds: Any,\n",
    "                 batch_size: int,\n",
    "                 mean: float = 0.,\n",
    "                 std: float = 1.):\n",
    "        \n",
    "        self.image_ds = image_ds\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_ds.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return ((self.image_ds.__getitem__(idx)[0].numpy() - mean) / std).transpose(1, 2, 0)\n",
    "\n",
    "    def get_dataloader(self, shuffle=True):\n",
    "        return DataLoader(\n",
    "            self,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "image_ds = ImageDataClass(mnist_ds, batch_size=128, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(hk.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, time):\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = jnp.log(10000) / (half_dim - 1)\n",
    "        embeddings = jnp.exp(jnp.arange(half_dim) * -embeddings)\n",
    "        embeddings = time[:, jnp.newaxis] * embeddings[jnp.newaxis, :]\n",
    "        embeddings = jnp.concatenate(\n",
    "            (jnp.sin(embeddings), jnp.cos(embeddings)), axis=-1\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "class Block(hk.Module):\n",
    "    def __init__(self, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = hk.Conv2D(dim_out, kernel_shape=3, padding=(1, 1))\n",
    "        self.norm = hk.GroupNorm(groups)\n",
    "        self.act = jax.nn.silu\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(hk.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
    "\n",
    "    def __init__(self, dim_out, groups=8, change_dim=False):\n",
    "        super().__init__()\n",
    "        self.mlp = hk.Sequential([jax.nn.silu, hk.Linear(dim_out)])\n",
    "        self.block1 = Block(dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, groups=groups)\n",
    "        self.res_conv = (\n",
    "            hk.Conv2D(dim_out, kernel_shape=1, padding=(0, 0))\n",
    "            if change_dim\n",
    "            else lambda x: x\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, time_emb):\n",
    "        h = self.block1(x)\n",
    "        time_emb = self.mlp(time_emb)\n",
    "        # We add new axes to the time embedding to for broadcasting.\n",
    "        h = time_emb[:, jnp.newaxis] + h\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "    \n",
    "\n",
    "def SpatialUpsample(dim):\n",
    "    return hk.Conv2DTranspose(dim, kernel_shape=4, stride=2)\n",
    "\n",
    "\n",
    "def SpatialDownsample(dim):\n",
    "    return hk.Conv2D(dim, kernel_shape=4, stride=2, padding=(1, 1))\n",
    "\n",
    "\n",
    "class Unet(hk.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_mults=(1,),\n",
    "        channels=1,\n",
    "        resnet_block_groups=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        init_dim = dim // 3 * 2\n",
    "        self.init_conv = hk.Conv2D(init_dim, kernel_shape=7, padding=(3, 3))\n",
    "\n",
    "        # time embeddings\n",
    "        time_dim = dim * 4\n",
    "        self.time_mlp = hk.Sequential(\n",
    "            [\n",
    "                TimeEmbedding(dim),\n",
    "                hk.Linear(time_dim),\n",
    "                jax.nn.gelu,\n",
    "                hk.Linear(time_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "        self.downs = []\n",
    "        dims = list(map(lambda m: dim * m, dim_mults))\n",
    "\n",
    "        for ind, stage_dim in enumerate(dims):\n",
    "            is_last = ind >= len(dims) - 1\n",
    "\n",
    "            self.downs.append(\n",
    "                [\n",
    "                    ResnetBlock(\n",
    "                        stage_dim, groups=resnet_block_groups, change_dim=True),\n",
    "                    ResnetBlock(stage_dim, groups=resnet_block_groups),\n",
    "                    # We don't apply spatial downsampling to the last stage. This is\n",
    "                    # because we go from 28x28 -> 14x14 -> 7x7 in the 1st and 2nd\n",
    "                    # stages and 7 can't be halved without a remainder, which\n",
    "                    # would cause problems in the upsampling path.\n",
    "                    SpatialDownsample(\n",
    "                        stage_dim) if not is_last else lambda x: x,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = ResnetBlock(mid_dim, groups=resnet_block_groups)\n",
    "        self.mid_block2 = ResnetBlock(mid_dim, groups=resnet_block_groups)\n",
    "\n",
    "        self.ups = []\n",
    "        rev_dims = list(reversed(dims))\n",
    "        for ind, stage_dim in enumerate(rev_dims):\n",
    "            is_last = ind >= len(rev_dims) - 1\n",
    "\n",
    "            self.ups.append(\n",
    "                [\n",
    "                    ResnetBlock(\n",
    "                        stage_dim, groups=resnet_block_groups, change_dim=True),\n",
    "                    ResnetBlock(stage_dim, groups=resnet_block_groups),\n",
    "                    SpatialUpsample(stage_dim) if not is_last else lambda x: x,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.final_block = ResnetBlock(dim, groups=resnet_block_groups)\n",
    "        self.final_conv = hk.Conv2D(channels, kernel_shape=1, padding=(0, 0))\n",
    "\n",
    "    def __call__(self, time, x):\n",
    "        x = self.init_conv(x)\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "        # downsample\n",
    "        for block1, block2, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "            \n",
    "\n",
    "        # bottleneck\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        # upsample\n",
    "        for block1, block2, upsample in self.ups:\n",
    "            x = jnp.concatenate((x, h.pop()), axis=-1)\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_block(x, t)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(key, batch_size=16, dimension=32):\n",
    "    \"\"\" Initialize a model. \"\"\"\n",
    "    def forward(t, x):\n",
    "        model = Unet(dim=32)\n",
    "        return model(t, x)\n",
    "    model = hk.transform(forward)\n",
    "\n",
    "    input_shape = (batch_size, dimension, dimension, 1)\n",
    "    t_shape = (batch_size, 1)\n",
    "    dummy_t = jnp.zeros(t_shape)\n",
    "    dummy_input = jnp.zeros(input_shape)\n",
    "\n",
    "    init_params = model.init(key, t=dummy_t, x=dummy_input)\n",
    "    return model, init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "sde = OrnsteinUhlenbeck(N=1000)\n",
    "model, init_params = init_model(key, batch_size=image_ds.batch_size, dimension=32)\n",
    "loss_fn = DSMLoss(alpha=0, diff_weight=False).loss_fn\n",
    "optimizer = optax.adam(learning_rate=1e-4, b1=.9, b2=0.999, eps=1e-8)\n",
    "step_fn = get_step_fn(loss_fn, optimizer, sde, model)\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class DiffusionState:\n",
    "    dataclass: Any\n",
    "    step_fn: Any\n",
    "\n",
    "diffusion = DiffusionState(dataclass=image_ds,\n",
    "                             step_fn=step_fn)\n",
    "\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class State:\n",
    "    step: int\n",
    "    opt_state: Any\n",
    "    params: Any\n",
    "    ema_rate: float\n",
    "    params_ema: Any\n",
    "\n",
    "\n",
    "step = 0\n",
    "opt_state = optimizer.init(init_params)\n",
    "params = init_params\n",
    "params_ema = init_params\n",
    "\n",
    "state = State(step=step,\n",
    "              opt_state=opt_state,\n",
    "              params=params,\n",
    "              ema_rate=0.99,\n",
    "              params_ema=params_ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_total = 50000\n",
    "state = train_diffusion(diffusion, state, steps_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "dataloader = image_ds.get_dataloader(shuffle=False)\n",
    "\n",
    "original = next(iter(dataloader)).reshape(-1, 1, 32, 32)\n",
    "noised = original * np.exp(-1) + np.random.normal(0, 1, original.shape) * np.sqrt(1 - np.exp(-2))\n",
    "\n",
    "images_original = torch.tensor(original.reshape(-1, 1, 32, 32))\n",
    "images_noised = torch.tensor(noised.reshape(-1, 1, 32, 32))\n",
    "\n",
    "def show_image_grid(images, nrow=8, padding=2,\n",
    "                    normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    from torchvision.utils import make_grid\n",
    "    grid = make_grid(images, nrow=nrow, padding=padding,\n",
    "                     normalize=normalize, range=range, scale_each=scale_each, pad_value=pad_value)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    \n",
    "\n",
    "show_image_grid(images_original, nrow=12, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_grid(images_noised, nrow=12, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_ds = TwoDimDataClass(dataset_type='gaussian_centered',\n",
    "                              N=500,\n",
    "                              dimension=1024,\n",
    "                              batch_size=256)\n",
    "\n",
    "sde_backward = sde.reverse(model, state.params_ema)\n",
    "sampler = Sampler(eps=1e-3)\n",
    "sampler_fn = sampler.get_sampling_fn(sde_backward, reference_ds)\n",
    "\n",
    "out, ntot, timesteps, x_hist = sampler_fn(key, N_samples=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.tensor(np.array(x_hist[-1].reshape(-1, 1, 32, 32)))\n",
    "show_image_grid(images, nrow=12, padding=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7557f7e64878364ce9b46792a54a0e279cfb46384047c79e408d27091ca2d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
